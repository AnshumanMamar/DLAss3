{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import heapq\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val=torch.cuda.is_available()\n",
    "if val == 1:\n",
    "    device= torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('gpu')\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(params):\n",
    "    dataset_path = params['dataset_path']\n",
    "    train_data = csv.reader(open(dataset_path + '/hin/hin_train.csv',encoding='utf8'))\n",
    "    val_data = csv.reader(open(dataset_path + '/hin/hin_valid.csv',encoding='utf8'))\n",
    "    test_data = csv.reader(open(dataset_path + '/hin/hin_test.csv',encoding='utf8'))\n",
    "    train_translations = []\n",
    "    test_words=[]\n",
    "    val_translations = []\n",
    "    val_words=[]\n",
    "    train_words =[]\n",
    "    test_translations = []\n",
    "    pad=''\n",
    "    start='$'\n",
    "    end ='&' \n",
    "    train_data_list = list(train_data)\n",
    "    train_len = len(train_data_list)\n",
    "    i = 0\n",
    "    while i < train_len:\n",
    "        pair = train_data_list[i]\n",
    "        train_words.append(pair[0] + end)\n",
    "        train_translations.append(start + pair[1] + end)\n",
    "        i += 1  \n",
    "    \n",
    "    i=0\n",
    "    val_data_list = list(val_data)\n",
    "    val_len = len(val_data_list)\n",
    "    while i < val_len :\n",
    "        pair=val_data_list[i]\n",
    "        val_words.append(pair[0]+end)\n",
    "        val_translations.append(start+pair[1]+end)\n",
    "        i+=1\n",
    "        \n",
    "    i=0\n",
    "    test_data_list = list(test_data)\n",
    "    test_len = len(test_data_list)\n",
    "    while i < test_len :\n",
    "        pair=test_data_list[i]\n",
    "        test_words.append(pair[0]+end)\n",
    "        test_translations.append(start+pair[1]+end)\n",
    "        i+=1   \n",
    "        \n",
    " \n",
    "    \n",
    "    test_words =np.array(test_words)\n",
    "    train_translations = np.array(train_translations)\n",
    "    val_translations =np.array(val_translations)\n",
    "    train_words = np.array(train_words)\n",
    "    test_translations = np.array(test_translations)\n",
    "    val_words =np.array(val_words)\n",
    "    \n",
    "    \n",
    "    \n",
    "    output_vocab,input_vocab = set() , set()\n",
    "    i = 0\n",
    "    word_len=len(train_words)\n",
    "    while i < word_len :\n",
    "        word = train_words[i]\n",
    "        character_index = 0\n",
    "        while character_index < len(word):\n",
    "            character = word[character_index]\n",
    "            input_vocab.add(character)\n",
    "            character_index += 1\n",
    "        i += 1         \n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    word_len=len(val_words)\n",
    "    while i < word_len :\n",
    "        word = val_words[i]\n",
    "        character_index = 0\n",
    "        while character_index < len(word):\n",
    "            character = word[character_index]\n",
    "            input_vocab.add(character)\n",
    "            character_index += 1\n",
    "        i += 1\n",
    "    \n",
    "    i = 0\n",
    "    word_len=len(test_words)\n",
    "    while i < word_len :\n",
    "        word = test_words[i]\n",
    "        character_index = 0\n",
    "        while character_index < len(word):\n",
    "            character = word[character_index]\n",
    "            input_vocab.add(character)\n",
    "            character_index += 1\n",
    "        i += 1\n",
    "\n",
    "    i = 0\n",
    "    word_len=len(train_translations)\n",
    "    while i < word_len :\n",
    "        word = train_translations[i]\n",
    "        character_index = 0\n",
    "        while character_index < len(word):\n",
    "            character = word[character_index]\n",
    "            output_vocab.add(character)\n",
    "            character_index += 1\n",
    "        i += 1\n",
    "        \n",
    "    i = 0\n",
    "    word_len=len(val_translations)\n",
    "    while i < word_len :\n",
    "        word = val_translations[i]\n",
    "        character_index = 0\n",
    "        while character_index < len(word):\n",
    "            character = word[character_index]\n",
    "            output_vocab.add(character)\n",
    "            character_index += 1\n",
    "        i += 1\n",
    "        \n",
    "    i = 0\n",
    "    word_len=len(test_translations)\n",
    "    while i < word_len :\n",
    "        word = test_translations[i]\n",
    "        character_index = 0\n",
    "        while character_index < len(word):\n",
    "            character = word[character_index]\n",
    "            output_vocab.add(character)\n",
    "            character_index += 1\n",
    "        i += 1\n",
    "    \n",
    "    output_vocab.remove(start)\n",
    "    input_vocab.remove(end)\n",
    "    output_vocab.remove(end)\n",
    "    \n",
    "    output_vocab= [pad, start, end] + list(sorted(output_vocab))\n",
    "    input_vocab = [pad, start, end] + list(sorted(input_vocab))\n",
    "            \n",
    " \n",
    "    output_index,input_index = {char: idx for idx, char in enumerate(output_vocab)},{char: idx for idx, char in enumerate(input_vocab)}\n",
    "    output_index_rev,input_index_rev = {idx: char for char, idx in output_index.items()},{idx: char for char, idx in input_index.items()}\n",
    "    \n",
    "\n",
    "    max_len = max(max([len(word) for word in np.hstack((train_words, test_words, val_words))]), max([len(word) for word in np.hstack((train_translations, val_translations, test_translations))]))\n",
    "        \n",
    "    preprocessed_data = {\n",
    "        'SOS' : start,\n",
    "        'EOS' : end,\n",
    "        'PAD' : pad,\n",
    "        'train_words' : train_words,\n",
    "        'train_translations' : train_translations,\n",
    "        'val_words' : val_words,\n",
    "        'val_translations' : val_translations,\n",
    "        'test_words' : test_words,\n",
    "        'test_translations' : test_translations,\n",
    "        'max_enc_len' : max([len(word) for word in np.hstack((train_words, test_words, val_words))]),\n",
    "        'max_dec_len' : max([len(word) for word in np.hstack((train_translations, val_translations, test_translations))]),\n",
    "        'max_len' : max_len,\n",
    "        'input_index' : input_index,\n",
    "        'output_index' : output_index,\n",
    "        'input_index_rev' : input_index_rev,\n",
    "        'output_index_rev' : output_index_rev\n",
    "    }\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor(preprocessed_data):\n",
    "    prop_data=preprocessed_data['max_len']\n",
    "    leng=len(preprocessed_data['train_words'])\n",
    "    d_type='int64'\n",
    "    input_data = np.zeros((prop_data,leng), dtype = d_type)\n",
    "    output_data = np.zeros((prop_data,leng), dtype = d_type)\n",
    "    leng=len(preprocessed_data['val_words'])\n",
    "    val_input_data = np.zeros((prop_data,leng), dtype = d_type)\n",
    "    val_output_data = np.zeros((prop_data,leng), dtype = d_type)\n",
    "    leng=len(preprocessed_data['test_words'])\n",
    "    test_input_data = np.zeros((prop_data,leng), dtype = d_type)\n",
    "    test_output_data = np.zeros((prop_data,leng), dtype = d_type)\n",
    "    \n",
    "\n",
    "    idx = 0\n",
    "    while idx < len(preprocessed_data['train_words']):\n",
    "        w = preprocessed_data['train_words'][idx]\n",
    "        t = preprocessed_data['train_translations'][idx]\n",
    "\n",
    "        i = 0\n",
    "        while i < len(w):\n",
    "            char = w[i]\n",
    "            input_data[i, idx] = preprocessed_data['input_index'][char]\n",
    "            i += 1\n",
    "\n",
    "        i = 0\n",
    "        while i < len(t):\n",
    "            char = t[i]\n",
    "            output_data[i, idx] = preprocessed_data['output_index'][char]\n",
    "            i += 1\n",
    "        idx += 1            \n",
    "        \n",
    "\n",
    "            \n",
    "    idx = 0\n",
    "    while idx < len(preprocessed_data['val_words']):\n",
    "        w = preprocessed_data['val_words'][idx]\n",
    "        t = preprocessed_data['val_translations'][idx]\n",
    "\n",
    "        i = 0\n",
    "        while i < len(w):\n",
    "            char = w[i]\n",
    "            val_input_data[i, idx] = preprocessed_data['input_index'][char]\n",
    "            i += 1\n",
    "\n",
    "        i = 0\n",
    "        while i < len(t):\n",
    "            char = t[i]\n",
    "            val_output_data[i, idx] = preprocessed_data['output_index'][char]\n",
    "            i += 1\n",
    "        idx += 1            \n",
    "        \n",
    "            \n",
    "    idx = 0\n",
    "    while idx < len(preprocessed_data['test_words']):\n",
    "        w = preprocessed_data['test_words'][idx]\n",
    "        t = preprocessed_data['test_translations'][idx]\n",
    "\n",
    "        i = 0\n",
    "        while i < len(w):\n",
    "            char = w[i]\n",
    "            test_input_data[i, idx] = preprocessed_data['input_index'][char]\n",
    "            i += 1\n",
    "\n",
    "        i = 0\n",
    "        while i < len(t):\n",
    "            char = t[i]\n",
    "            test_output_data[i, idx] = preprocessed_data['output_index'][char]\n",
    "            i += 1\n",
    "        idx += 1            \n",
    "        \n",
    "            \n",
    "            \n",
    "    output_data=torch.tensor(output_data, dtype = torch.int64)\n",
    "    input_data = torch.tensor(input_data,dtype = torch.int64)\n",
    "    val_output_data=torch.tensor(val_output_data, dtype = torch.int64)\n",
    "    val_input_data = torch.tensor(val_input_data,dtype = torch.int64)\n",
    "    test_output_data=torch.tensor(test_output_data, dtype = torch.int64)\n",
    "    test_input_data= torch.tensor(test_input_data,dtype = torch.int64)\n",
    "    \n",
    "    tensors = {\n",
    "        'input_data' : input_data,\n",
    "        'output_data' : output_data,\n",
    "        'val_input_data' : val_input_data,\n",
    "        'val_output_data' : val_output_data, \n",
    "        'test_input_data' : test_input_data,\n",
    "        'test_output_data' : test_output_data\n",
    "    }\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        val=hidden_size\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = val\n",
    "    def dot_score(self,hidden_state, encoder_states):\n",
    "        cal = torch.sum(hidden_state * encoder_states, dim=2)\n",
    "        return cal\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        cal = F.softmax(self.dot_score(hidden, encoder_outputs).t(), dim=1).unsqueeze(1)\n",
    "        return cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, params, preprocessed_data):\n",
    "        hd='hidden_size'\n",
    "        dp='dropout'\n",
    "        super(Encoder_Attention, self).__init__() \n",
    "        self.bi_directional,self.cell_type = params['bi_dir'],params['cell_type']\n",
    "        leng=len(preprocessed_data['input_index'])\n",
    "        self.dropout,self.embedding = nn.Dropout(params[dp]),nn.Embedding(leng, params[es])\n",
    "        self.hidden_size = params[hd]\n",
    "        val=self.cell_type\n",
    "        nm='num_layers_enc'\n",
    "        es='embedding_size' \n",
    "        if val == 'GRU':\n",
    "            self.cell = nn.GRU(params[es], params[hd], params[nm], dropout = params[dp], bidirectional = self.bi_directional)\n",
    "        if val == 'RNN':\n",
    "            self.cell = nn.RNN(params[es], params[hd], params[nm], dropout = params[dp], bidirectional = self.bi_directional)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoder_states, hidden = self.cell(self.dropout(self.embedding(x)))\n",
    "        val=self.bi_directional\n",
    "        if val:\n",
    "            encoder_states = encoder_states[:, :, :self.hidden_size] + encoder_states[:, : ,self.hidden_size:]\n",
    "        return encoder_states, hidden"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
