{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import wandb\n",
    "import math\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val=torch.cuda.is_available()\n",
    "if val == 1:\n",
    "    device= torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('gpu')\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb login apni_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(params):\n",
    "    dataset_path = params['dataset_path']\n",
    "    train_data = csv.reader(open(dataset_path + '/hin/hin_train.csv',encoding='utf8'))\n",
    "    val_data = csv.reader(open(dataset_path + '/hin/hin_valid.csv',encoding='utf8'))\n",
    "    test_data = csv.reader(open(dataset_path + '/hin/hin_test.csv',encoding='utf8'))\n",
    "    train_translations = []\n",
    "    test_words=[]\n",
    "    val_translations = []\n",
    "    val_words=[]\n",
    "    train_words =[]\n",
    "    test_translations = []\n",
    "    pad=''\n",
    "    start='$'\n",
    "    end ='&' \n",
    "    train_data_list = list(train_data)\n",
    "    train_len = len(train_data_list)\n",
    "    i = 0\n",
    "    while i < train_len:\n",
    "        pair = train_data_list[i]\n",
    "        train_words.append(pair[0] + end)\n",
    "        train_translations.append(start + pair[1] + end)\n",
    "        i += 1  \n",
    "    \n",
    "    i=0\n",
    "    val_data_list = list(val_data)\n",
    "    val_len = len(val_data_list)\n",
    "    while i < val_len :\n",
    "        pair=val_data_list[i]\n",
    "        val_words.append(pair[0]+end)\n",
    "        val_translations.append(start+pair[1]+end)\n",
    "        i+=1\n",
    "        \n",
    "    i=0\n",
    "    test_data_list = list(test_data)\n",
    "    test_len = len(test_data_list)\n",
    "    while i < test_len :\n",
    "        pair=test_data_list[i]\n",
    "        test_words.append(pair[0]+end)\n",
    "        test_translations.append(start+pair[1]+end)\n",
    "        i+=1   \n",
    "        \n",
    " \n",
    "    \n",
    "    test_words =np.array(test_words)\n",
    "    train_translations = np.array(train_translations)\n",
    "    val_translations =np.array(val_translations)\n",
    "    train_words = np.array(train_words)\n",
    "    test_translations = np.array(test_translations)\n",
    "    val_words =np.array(val_words)\n",
    "    \n",
    "    \n",
    "    \n",
    "    output_vocab,input_vocab = set() , set()\n",
    "    i = 0\n",
    "    word_len=len(train_words)\n",
    "    while i < word_len :\n",
    "        word = train_words[i]\n",
    "        character_index = 0\n",
    "        while character_index < len(word):\n",
    "            character = word[character_index]\n",
    "            input_vocab.add(character)\n",
    "            character_index += 1\n",
    "        i += 1         \n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    word_len=len(val_words)\n",
    "    while i < word_len :\n",
    "        word = val_words[i]\n",
    "        character_index = 0\n",
    "        while character_index < len(word):\n",
    "            character = word[character_index]\n",
    "            input_vocab.add(character)\n",
    "            character_index += 1\n",
    "        i += 1\n",
    "    \n",
    "    i = 0\n",
    "    word_len=len(test_words)\n",
    "    while i < word_len :\n",
    "        word = test_words[i]\n",
    "        character_index = 0\n",
    "        while character_index < len(word):\n",
    "            character = word[character_index]\n",
    "            input_vocab.add(character)\n",
    "            character_index += 1\n",
    "        i += 1\n",
    "\n",
    "    i = 0\n",
    "    word_len=len(train_translations)\n",
    "    while i < word_len :\n",
    "        word = train_translations[i]\n",
    "        character_index = 0\n",
    "        while character_index < len(word):\n",
    "            character = word[character_index]\n",
    "            output_vocab.add(character)\n",
    "            character_index += 1\n",
    "        i += 1\n",
    "        \n",
    "    i = 0\n",
    "    word_len=len(val_translations)\n",
    "    while i < word_len :\n",
    "        word = val_translations[i]\n",
    "        character_index = 0\n",
    "        while character_index < len(word):\n",
    "            character = word[character_index]\n",
    "            output_vocab.add(character)\n",
    "            character_index += 1\n",
    "        i += 1\n",
    "        \n",
    "    i = 0\n",
    "    word_len=len(test_translations)\n",
    "    while i < word_len :\n",
    "        word = test_translations[i]\n",
    "        character_index = 0\n",
    "        while character_index < len(word):\n",
    "            character = word[character_index]\n",
    "            output_vocab.add(character)\n",
    "            character_index += 1\n",
    "        i += 1\n",
    "    \n",
    "    output_vocab.remove(start)\n",
    "    input_vocab.remove(end)\n",
    "    output_vocab.remove(end)\n",
    "    \n",
    "    output_vocab= [pad, start, end] + list(sorted(output_vocab))\n",
    "    input_vocab = [pad, start, end] + list(sorted(input_vocab))\n",
    "            \n",
    " \n",
    "    output_index,input_index = {char: idx for idx, char in enumerate(output_vocab)},{char: idx for idx, char in enumerate(input_vocab)}\n",
    "    output_index_rev,input_index_rev = {idx: char for char, idx in output_index.items()},{idx: char for char, idx in input_index.items()}\n",
    "    \n",
    "\n",
    "    max_len = max(max([len(word) for word in np.hstack((train_words, test_words, val_words))]), max([len(word) for word in np.hstack((train_translations, val_translations, test_translations))]))\n",
    "        \n",
    "    preprocessed_data = {\n",
    "        'SOS' : start,\n",
    "        'EOS' : end,\n",
    "        'PAD' : pad,\n",
    "        'train_words' : train_words,\n",
    "        'train_translations' : train_translations,\n",
    "        'val_words' : val_words,\n",
    "        'val_translations' : val_translations,\n",
    "        'test_words' : test_words,\n",
    "        'test_translations' : test_translations,\n",
    "        'max_enc_len' : max([len(word) for word in np.hstack((train_words, test_words, val_words))]),\n",
    "        'max_dec_len' : max([len(word) for word in np.hstack((train_translations, val_translations, test_translations))]),\n",
    "        'max_len' : max_len,\n",
    "        'input_index' : input_index,\n",
    "        'output_index' : output_index,\n",
    "        'input_index_rev' : input_index_rev,\n",
    "        'output_index_rev' : output_index_rev\n",
    "    }\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor(preprocessed_data):\n",
    "    prop_data=preprocessed_data['max_len']\n",
    "    leng=len(preprocessed_data['train_words'])\n",
    "    d_type='int64'\n",
    "    input_data = np.zeros((prop_data,leng), dtype = d_type)\n",
    "    output_data = np.zeros((prop_data,leng), dtype = d_type)\n",
    "    leng=len(preprocessed_data['val_words'])\n",
    "    val_input_data = np.zeros((prop_data,leng), dtype = d_type)\n",
    "    val_output_data = np.zeros((prop_data,leng), dtype = d_type)\n",
    "    leng=len(preprocessed_data['test_words'])\n",
    "    test_input_data = np.zeros((prop_data,leng), dtype = d_type)\n",
    "    test_output_data = np.zeros((prop_data,leng), dtype = d_type)\n",
    "    \n",
    "\n",
    "    idx = 0\n",
    "    while idx < len(preprocessed_data['train_words']):\n",
    "        w = preprocessed_data['train_words'][idx]\n",
    "        t = preprocessed_data['train_translations'][idx]\n",
    "\n",
    "        i = 0\n",
    "        while i < len(w):\n",
    "            char = w[i]\n",
    "            input_data[i, idx] = preprocessed_data['input_index'][char]\n",
    "            i += 1\n",
    "\n",
    "        i = 0\n",
    "        while i < len(t):\n",
    "            char = t[i]\n",
    "            output_data[i, idx] = preprocessed_data['output_index'][char]\n",
    "            i += 1\n",
    "        idx += 1            \n",
    "        \n",
    "\n",
    "            \n",
    "    idx = 0\n",
    "    while idx < len(preprocessed_data['val_words']):\n",
    "        w = preprocessed_data['val_words'][idx]\n",
    "        t = preprocessed_data['val_translations'][idx]\n",
    "\n",
    "        i = 0\n",
    "        while i < len(w):\n",
    "            char = w[i]\n",
    "            val_input_data[i, idx] = preprocessed_data['input_index'][char]\n",
    "            i += 1\n",
    "\n",
    "        i = 0\n",
    "        while i < len(t):\n",
    "            char = t[i]\n",
    "            val_output_data[i, idx] = preprocessed_data['output_index'][char]\n",
    "            i += 1\n",
    "        idx += 1            \n",
    "        \n",
    "            \n",
    "    idx = 0\n",
    "    while idx < len(preprocessed_data['test_words']):\n",
    "        w = preprocessed_data['test_words'][idx]\n",
    "        t = preprocessed_data['test_translations'][idx]\n",
    "\n",
    "        i = 0\n",
    "        while i < len(w):\n",
    "            char = w[i]\n",
    "            test_input_data[i, idx] = preprocessed_data['input_index'][char]\n",
    "            i += 1\n",
    "\n",
    "        i = 0\n",
    "        while i < len(t):\n",
    "            char = t[i]\n",
    "            test_output_data[i, idx] = preprocessed_data['output_index'][char]\n",
    "            i += 1\n",
    "        idx += 1            \n",
    "        \n",
    "            \n",
    "            \n",
    "    output_data=torch.tensor(output_data, dtype = torch.int64)\n",
    "    input_data = torch.tensor(input_data,dtype = torch.int64)\n",
    "    val_output_data=torch.tensor(val_output_data, dtype = torch.int64)\n",
    "    val_input_data = torch.tensor(val_input_data,dtype = torch.int64)\n",
    "    test_output_data=torch.tensor(test_output_data, dtype = torch.int64)\n",
    "    test_input_data= torch.tensor(test_input_data,dtype = torch.int64)\n",
    "    \n",
    "    tensors = {\n",
    "        'input_data' : input_data,\n",
    "        'output_data' : output_data,\n",
    "        'val_input_data' : val_input_data,\n",
    "        'val_output_data' : val_output_data, \n",
    "        'test_input_data' : test_input_data,\n",
    "        'test_output_data' : test_output_data\n",
    "    }\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module): ####################################\n",
    "    def __init__(self, params, preprocessed_data):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.cell_type = params['cell_type']\n",
    "        self.dropout = nn.Dropout(params['dropout'])\n",
    "        self.embedding = nn.Embedding(len(preprocessed_data['input_index']), params['embedding_size'])\n",
    "        if self.cell_type == 'RNN':\n",
    "            self.cell = nn.RNN(params['embedding_size'], params['hidden_size'], params['num_layers_enc'], dropout = params['dropout'], bidirectional = params['bi_dir'])\n",
    "        elif self.cell_type == 'GRU':\n",
    "            self.cell = nn.GRU(params['embedding_size'], params['hidden_size'], params['num_layers_enc'], dropout = params['dropout'], bidirectional = params['bi_dir'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        drop_par = self.embedding(x)\n",
    "        _ , hidden = self.cell(self.dropout(drop_par))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):####################################\n",
    "    def __init__(self, params, preprocessed_data):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.cell_type = params['cell_type']\n",
    "        self.dropout = nn.Dropout(params['dropout'])\n",
    "        self.embedding = nn.Embedding(len(preprocessed_data['output_index']), params['embedding_size'])\n",
    "        if self.cell_type == 'RNN':\n",
    "            self.cell = nn.RNN(params['embedding_size'], params['hidden_size'], params['num_layers_dec'], dropout = params['dropout'], bidirectional = params['bi_dir'])\n",
    "        elif self.cell_type == 'GRU':\n",
    "            self.cell = nn.GRU(params['embedding_size'], params['hidden_size'], params['num_layers_dec'], dropout = params['dropout'], bidirectional = params['bi_dir'])\n",
    "        self.fc = nn.Linear(params['hidden_size'] * 2 if params['bi_dir'] == True else params['hidden_size'], len(preprocessed_data['output_index']))\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        embedding = self.embedding(x.unsqueeze(0))\n",
    "        outputs, hidden = self.cell(self.dropout(embedding), hidden)\n",
    "        predictions = self.fc(outputs).squeeze(0)\n",
    "        return predictions, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):####################################\n",
    "    def __init__(self, encoder, decoder, params,  preprocessed_data):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.cell_type = params['cell_type']\n",
    "        self.decoder, self.encoder  = decoder, encoder\n",
    "        self.output_index_len = len(preprocessed_data['output_index'])\n",
    "        self.tfr = params['teacher_fr']\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        batch_size, target_len = source.shape[1], target.shape[0]\n",
    "        x = target[0]\n",
    "        outputs = torch.zeros(target_len, batch_size, self.output_index_len).to(device)\n",
    "        hidden = self.encoder(source)\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden = self.decoder(x, hidden, None)\n",
    "            outputs[t], best_guess = output, output.argmax(1)\n",
    "            x = best_guess if random.random() >= self.tfr else target[t]\n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
