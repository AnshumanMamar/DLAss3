{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"2c353da7-e667-45b4-a5e8-e6e90a4b765b","_uuid":"4fd0063d-279d-4e3f-9fa0-18f2574f3925","trusted":true},"source":["# IMPORTS"]},{"cell_type":"code","execution_count":21,"metadata":{"_cell_guid":"2e4ce556-2420-4483-a4d7-26b1ae6031bb","_uuid":"9414a05c-f79e-4034-8d25-e4dece549e46","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:55:43.781588Z","iopub.status.busy":"2024-05-16T17:55:43.781192Z","iopub.status.idle":"2024-05-16T17:55:43.789247Z","shell.execute_reply":"2024-05-16T17:55:43.786661Z","shell.execute_reply.started":"2024-05-16T17:55:43.781555Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Import necessary libraries\n","import numpy as np\n","import csv\n","import torch\n","import pandas as pd\n","import torch.nn as nn\n","import os\n","import heapq\n","from tqdm import tqdm\n","import torch.optim as optim\n","import random\n","import math\n","import torch.nn.functional as F\n","import warnings\n","import wandb\n","from torch.nn.utils import clip_grad_norm_\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":22,"metadata":{"_cell_guid":"085555d5-c94e-497d-85a2-72d74ba96ebb","_uuid":"ea737d35-7f0e-41fc-8687-501c6ec03aef","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:55:43.791149Z","iopub.status.busy":"2024-05-16T17:55:43.790868Z","iopub.status.idle":"2024-05-16T17:55:43.804164Z","shell.execute_reply":"2024-05-16T17:55:43.803073Z","shell.execute_reply.started":"2024-05-16T17:55:43.791124Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Check if CUDA is available\n","val=torch.cuda.is_available()\n","if val == 1:\n","    \n"," # If CUDA is available, use it as the device    \n","    device= torch.device('cuda')\n","else:\n","    \n","# If GPU is also unavailable, default to CPU\n","    device = torch.device('gpu')\n","    "]},{"cell_type":"code","execution_count":23,"metadata":{"_cell_guid":"28350379-6d42-4904-8d63-a18a7aadb9af","_uuid":"f597efc3-31a5-4176-a73b-3f8656015a5e","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:55:43.805658Z","iopub.status.busy":"2024-05-16T17:55:43.805367Z","iopub.status.idle":"2024-05-16T17:55:43.815423Z","shell.execute_reply":"2024-05-16T17:55:43.814551Z","shell.execute_reply.started":"2024-05-16T17:55:43.805633Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# !wandb login f63b589f2c31fd6562d752168e172d22870ab562"]},{"cell_type":"markdown","metadata":{"_cell_guid":"04196958-6040-481e-b878-425f053bf281","_uuid":"0272b181-3afc-4777-8bc0-c71ae9dac9c0","trusted":true},"source":["# PREPROCESSING"]},{"cell_type":"code","execution_count":24,"metadata":{"_cell_guid":"63055d72-935e-4e29-ba09-6b40e7e837fa","_uuid":"9ce96016-d22c-4215-9ef6-244ac032c884","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:55:43.817747Z","iopub.status.busy":"2024-05-16T17:55:43.817444Z","iopub.status.idle":"2024-05-16T17:55:43.844046Z","shell.execute_reply":"2024-05-16T17:55:43.843274Z","shell.execute_reply.started":"2024-05-16T17:55:43.817719Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# This function loads and preprocesses the dataset for training a sequence-to-sequence model.\n","def loadData(params):\n","    \n","    # Define path to dataset based on configuration\n","    data_path = params['dataset_path']\n","    \n","    # Open data files for training, validation, and testing\n","    tr_data = csv.reader(open(data_path + '/hin/hin_train.csv',encoding='utf8'))\n","    vl_data = csv.reader(open(data_path + '/hin/hin_valid.csv',encoding='utf8'))\n","    tt_data = csv.reader(open(data_path + '/hin/hin_test.csv',encoding='utf8'))\n","    \n","    # Initialize empty lists to store data\n","    tr_translations = []\n","    tt_words=[]\n","    vl_translations = []\n","    vl_words=[]\n","    tr_words =[]\n","    tt_translations = []\n","    pad=''\n","    start='$'\n","    end ='&'\n","    \n","    # Load training data\n","    train_data_list = list(tr_data)\n","    train_len = len(train_data_list)\n","    i = 0\n","    while i < train_len:\n","        pair = train_data_list[i]\n","        tr_words.append(pair[0] + end)\n","        tr_translations.append(start + pair[1] + end)\n","        i += 1  \n","    \n","    # Load validation data \n","    i=0\n","    val_data_list = list(vl_data)\n","    val_len = len(val_data_list)\n","    while i < val_len :\n","        pair=val_data_list[i]\n","        vl_words.append(pair[0]+end)\n","        vl_translations.append(start+pair[1]+end)\n","        i+=1\n","    \n","    # Load validation data   \n","    i=0\n","    test_data_list = list(tt_data)\n","    test_len = len(test_data_list)\n","    while i < test_len :\n","        pair=test_data_list[i]\n","        tt_words.append(pair[0]+end)\n","        tt_translations.append(start+pair[1]+end)\n","        i+=1   \n","        \n"," \n","    # Convert lists to NumPy arrays for better performance\n","    tt_words =np.array(tt_words)\n","    tr_translations = np.array(tr_translations)\n","    vl_translations =np.array(vl_translations)\n","    tr_words = np.array(tr_words)\n","    tt_translations = np.array(tt_translations)\n","    vl_words =np.array(vl_words)\n","    \n","    \n","    # Build input and output vocabularies\n","    output_vocab,input_vocab = set() , set()\n","    \n","    # Add characters from train_words to input_vocab\n","    i = 0\n","    word_len=len(tr_words)\n","    while i < word_len :\n","        word = tr_words[i]\n","        char_index = 0\n","        while char_index < len(word):\n","            character = word[char_index]\n","            input_vocab.add(character)\n","            char_index += 1\n","        i += 1         \n","    \n","    # Add characters from val_words to input_vocab\n","    i = 0\n","    word_len=len(vl_words)\n","    while i < word_len :\n","        word = vl_words[i]\n","        char_index = 0\n","        while char_index < len(word):\n","            character = word[char_index]\n","            input_vocab.add(character)\n","            char_index += 1\n","        i += 1\n","        \n","    # Add characters from test_words to input_vocab\n","    i = 0\n","    word_len=len(tt_words)\n","    while i < word_len :\n","        word = tt_words[i]\n","        char_index = 0\n","        while char_index < len(word):\n","            character = word[char_index]\n","            input_vocab.add(character)\n","            char_index += 1\n","        i += 1\n","    \n","    # Add characters from train_translations, val_translations, and test_translations to output_vocab\n","    i = 0\n","    word_len=len(tr_translations)\n","    while i < word_len :\n","        word = tr_translations[i]\n","        char_index = 0\n","        while char_index < len(word):\n","            character = word[char_index]\n","            output_vocab.add(character)\n","            char_index += 1\n","        i += 1\n","        \n","    i = 0\n","    word_len=len(vl_translations)\n","    while i < word_len :\n","        word = vl_translations[i]\n","        char_index = 0\n","        while char_index < len(word):\n","            character = word[char_index]\n","            output_vocab.add(character)\n","            char_index += 1\n","        i += 1\n","        \n","    i = 0\n","    word_len=len(tt_translations)\n","    while i < word_len :\n","        word = tt_translations[i]\n","        char_index = 0\n","        while char_index < len(word):\n","            character = word[char_index]\n","            output_vocab.add(character)\n","            char_index += 1\n","        i += 1\n","    # Remove special tokens from output_vocab\n","    output_vocab.remove(start)\n","    input_vocab.remove(end)\n","    output_vocab.remove(end)\n","    \n","    # Sort vocabularies and add special tokens\n","    output_vocab= [pad, start, end] + list(sorted(output_vocab))\n","    input_vocab = [pad, start, end] + list(sorted(input_vocab))\n","            \n","    # Create index mappings for vocabularies\n","    output_index,input_index = {char: idx for idx, char in enumerate(output_vocab)},{char: idx for idx, char in enumerate(input_vocab)}\n","    output_index_rev,input_index_rev = {idx: char for char, idx in output_index.items()},{idx: char for char, idx in input_index.items()}\n","    \n","    # Determine maximum sequence length\n","    max_len = max(max([len(word) for word in np.hstack((tr_words, tt_words, vl_words))]), max([len(word) for word in np.hstack((tr_translations, vl_translations, tt_translations))]))\n","    \n","    # Prepare preprocessed data dictionary\n","    preprocessed_data = {\n","        'SOS' : start,\n","        'EOS' : end,\n","        'PAD' : pad,\n","        'train_words' : tr_words,\n","        'train_translations' : tr_translations,\n","        'val_words' : vl_words,\n","        'val_translations' : vl_translations,\n","        'test_words' : tt_words,\n","        'test_translations' : tt_translations,\n","        'max_enc_len' : max([len(word) for word in np.hstack((tr_words, tt_words, vl_words))]),\n","        'max_dec_len' : max([len(word) for word in np.hstack((tr_translations, vl_translations, tt_translations))]),\n","        'max_len' : max_len,\n","        'input_index' : input_index,\n","        'output_index' : output_index,\n","        'input_index_rev' : input_index_rev,\n","        'output_index_rev' : output_index_rev\n","    }\n","    return preprocessed_data"]},{"cell_type":"code","execution_count":25,"metadata":{"_cell_guid":"669f82b7-d4b5-456a-beb6-80f16de1e40d","_uuid":"3b481d0a-92c2-4dff-a652-bd8447eb86bb","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:55:43.845649Z","iopub.status.busy":"2024-05-16T17:55:43.845394Z","iopub.status.idle":"2024-05-16T17:55:43.862998Z","shell.execute_reply":"2024-05-16T17:55:43.862125Z","shell.execute_reply.started":"2024-05-16T17:55:43.845627Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def create_tensor(preprocessed_data):\n","    \n","    # Extract max sequence length and the number of training examples\n","    prop_data=preprocessed_data['max_len']\n","    leng=len(preprocessed_data['train_words'])\n","    d_type='int64'\n","    \n","    # Initialize arrays for data\n","    input_data = np.zeros((prop_data,leng), dtype = d_type)\n","    output_data = np.zeros((prop_data,leng), dtype = d_type)\n","    leng=len(preprocessed_data['val_words'])\n","    vl_input_data = np.zeros((prop_data,leng), dtype = d_type)\n","    vl_output_data = np.zeros((prop_data,leng), dtype = d_type)\n","    leng=len(preprocessed_data['test_words'])\n","    tt_input_data = np.zeros((prop_data,leng), dtype = d_type)\n","    tt_output_data = np.zeros((prop_data,leng), dtype = d_type)\n","    \n","    # Fill in training data arrays\n","    idx = 0\n","    while idx < len(preprocessed_data['train_words']):\n","        w = preprocessed_data['train_words'][idx]\n","        t = preprocessed_data['train_translations'][idx]\n","\n","        i = 0\n","        while i < len(w):\n","            char = w[i]\n","            input_data[i, idx] = preprocessed_data['input_index'][char]\n","            i += 1\n","\n","        i = 0\n","        while i < len(t):\n","            char = t[i]\n","            output_data[i, idx] = preprocessed_data['output_index'][char]\n","            i += 1\n","        idx += 1            \n","        \n","\n","    # Fill in validation data arrays        \n","    idx = 0\n","    while idx < len(preprocessed_data['val_words']):\n","        w = preprocessed_data['val_words'][idx]\n","        t = preprocessed_data['val_translations'][idx]\n","\n","        i = 0\n","        while i < len(w):\n","            char = w[i]\n","            vl_input_data[i, idx] = preprocessed_data['input_index'][char]\n","            i += 1\n","\n","        i = 0\n","        while i < len(t):\n","            char = t[i]\n","            vl_output_data[i, idx] = preprocessed_data['output_index'][char]\n","            i += 1\n","        idx += 1            \n","        \n","    # Fill in test data arrays        \n","    idx = 0\n","    while idx < len(preprocessed_data['test_words']):\n","        w = preprocessed_data['test_words'][idx]\n","        t = preprocessed_data['test_translations'][idx]\n","\n","        i = 0\n","        while i < len(w):\n","            char = w[i]\n","            tt_input_data[i, idx] = preprocessed_data['input_index'][char]\n","            i += 1\n","\n","        i = 0\n","        while i < len(t):\n","            char = t[i]\n","            tt_output_data[i, idx] = preprocessed_data['output_index'][char]\n","            i += 1\n","        idx += 1            \n","        \n","            \n","    # Convert NumPy arrays to PyTorch tensors        \n","    output_data=torch.tensor(output_data, dtype = torch.int64)\n","    input_data = torch.tensor(input_data,dtype = torch.int64)\n","    vl_output_data=torch.tensor(vl_output_data, dtype = torch.int64)\n","    vl_input_data = torch.tensor(vl_input_data,dtype = torch.int64)\n","    tt_output_data=torch.tensor(tt_output_data, dtype = torch.int64)\n","    tt_input_data= torch.tensor(tt_input_data,dtype = torch.int64)\n","    \n","    #Store tensors in a dictionary\n","    tensors = {\n","        'input_data' : input_data,\n","        'output_data' : output_data,\n","        'val_input_data' : vl_input_data,\n","        'val_output_data' : vl_output_data, \n","        'test_input_data' : tt_input_data,\n","        'test_output_data' : tt_output_data\n","    }\n","    return tensors"]},{"cell_type":"markdown","metadata":{"_cell_guid":"5fdf866a-161b-493e-9990-79e40b0b6632","_uuid":"a4811e4a-6c89-428a-9fd7-ba21df8b2262","trusted":true},"source":["# ENCODER RG"]},{"cell_type":"code","execution_count":26,"metadata":{"_cell_guid":"5068b59d-e409-4127-bbe6-d95b872e7f80","_uuid":"1bc9a4e5-5217-4fc1-9d77-0612d014fcf3","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:55:43.864336Z","iopub.status.busy":"2024-05-16T17:55:43.864026Z","iopub.status.idle":"2024-05-16T17:55:43.878630Z","shell.execute_reply":"2024-05-16T17:55:43.877764Z","shell.execute_reply.started":"2024-05-16T17:55:43.864312Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Encoder module for a sequence-to-sequence model.\n","class Encoder(nn.Module): \n","    \n","    #Initializes the Encoder module.\n","    def __init__(self, params, preprocessed_data):\n","        super(Encoder, self).__init__()\n","        \n","        # Extract parameters\n","        self.cell_type = params['cell_type']\n","        self.dropout = nn.Dropout(params['dropout'])\n","        \n","        # Embedding layer\n","        self.embedding = nn.Embedding(len(preprocessed_data['input_index']), params['embedding_size'])\n","        \n","        # RNN or GRU cell based on cell_type\n","        if self.cell_type == 'RNN':\n","            self.cell = nn.RNN(params['embedding_size'], params['hidden_size'], params['num_layers_enc'], dropout = params['dropout'], bidirectional = params['bi_dir'])\n","        elif self.cell_type == 'GRU':\n","            self.cell = nn.GRU(params['embedding_size'], params['hidden_size'], params['num_layers_enc'], dropout = params['dropout'], bidirectional = params['bi_dir'])\n","    #Forward pass of the Encoder    \n","    def forward(self, x):\n","        \n","        # Embedding layer\n","        drop_par = self.embedding(x)\n","        # Pass through RNN/GRU cell\n","        _ , hidden = self.cell(self.dropout(drop_par))\n","        \n","        # Return hidden state\n","        return hidden"]},{"cell_type":"markdown","metadata":{"_cell_guid":"e84bfa59-4dee-4fe3-9448-46816d92caed","_uuid":"9a15c0af-0f32-4846-a503-33bf42d1cb5f","trusted":true},"source":["# DECODER RG"]},{"cell_type":"code","execution_count":27,"metadata":{"_cell_guid":"b8947fc9-8fbe-4a54-aca1-b578738909e2","_uuid":"b06cba4d-7131-4778-9d7c-963889880e0d","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:55:43.879871Z","iopub.status.busy":"2024-05-16T17:55:43.879619Z","iopub.status.idle":"2024-05-16T17:55:43.893848Z","shell.execute_reply":"2024-05-16T17:55:43.892939Z","shell.execute_reply.started":"2024-05-16T17:55:43.879849Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["#Decoder module for a sequence-to-sequence model.\n","class Decoder(nn.Module):\n","    \n","    #Initializes the Decoder module\n","    def __init__(self, params, preprocessed_data):\n","        super(Decoder, self).__init__()\n","        \n","        # Extract parameters\n","        self.cell_type = params['cell_type']\n","        self.dropout = nn.Dropout(params['dropout'])\n","        self.embedding = nn.Embedding(len(preprocessed_data['output_index']), params['embedding_size'])\n","        \n","        # RNN or GRU cell based on cell_type\n","        if self.cell_type == 'RNN':\n","            self.cell = nn.RNN(params['embedding_size'], params['hidden_size'], params['num_layers_dec'], dropout = params['dropout'], bidirectional = params['bi_dir'])\n","        elif self.cell_type == 'GRU':\n","            self.cell = nn.GRU(params['embedding_size'], params['hidden_size'], params['num_layers_dec'], dropout = params['dropout'], bidirectional = params['bi_dir'])\n","        \n","        # Fully connected layer for output prediction\n","        self.fc = nn.Linear(params['hidden_size'] * 2 if params['bi_dir'] == True else params['hidden_size'], len(preprocessed_data['output_index']))\n","    \n","    #Forward pass of the Decoder.\n","    def forward(self, x, hidden, cell):\n","        \n","        # Embedding layer\n","        emb = self.embedding(x.unsqueeze(0))\n","        outputs, hidden = self.cell(self.dropout(emb), hidden)\n","        \n","        # Predictions with fully connected layer\n","        pred = self.fc(outputs).squeeze(0)\n","        \n","        # Return predictions and updated hidden state\n","        return pred, hidden"]},{"cell_type":"markdown","metadata":{"_cell_guid":"644d2981-4165-4bee-af1e-102f7d0902ba","_uuid":"0bdc67d2-c1c1-4c3f-99fc-e98bff7f1017","trusted":true},"source":["# SEQ 2 SEQ RG"]},{"cell_type":"code","execution_count":28,"metadata":{"_cell_guid":"d0d16630-6efb-42a6-9350-ea55a9ae2577","_uuid":"661268fa-1df7-43c4-b423-73d628f20f86","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:55:43.895272Z","iopub.status.busy":"2024-05-16T17:55:43.894933Z","iopub.status.idle":"2024-05-16T17:55:43.907847Z","shell.execute_reply":"2024-05-16T17:55:43.907069Z","shell.execute_reply.started":"2024-05-16T17:55:43.895237Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["#Sequence-to-sequence model consisting of an Encoder and a Decoder.\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, params,  preprocessed_data):\n","        #Initializes the Seq2Seq model\n","        super(Seq2Seq, self).__init__()\n","        \n","        # Extract parameters\n","        self.cell_type = params['cell_type']\n","        self.decoder, self.encoder  = decoder, encoder\n","        self.output_index_len = len(preprocessed_data['output_index'])\n","        self.tfr = params['teacher_fr']\n","    \n","    #Forward pass of the Seq2Seq model\n","    def forward(self, source, target):\n","        \n","        # Extract batch size and target sequence length\n","        bs, target_len = source.shape[1], target.shape[0]\n","        x = target[0]\n","        outputs = torch.zeros(target_len, bs, self.output_index_len).to(device)\n","        \n","        # Encode the source sequence to obtain the initial hidden state\n","        hidden = self.encoder(source)\n","        \n","        # Iterate over each step in the target sequence\n","        for t in range(1, target_len):\n","            output, hidden = self.decoder(x, hidden, None)\n","            \n","            # Store the decoder output in the outputs tensor\n","            outputs[t], best_guess = output, output.argmax(1)\n","            \n","            # Determine the next input (x) for the decoder\n","            x = best_guess if random.random() >= self.tfr else target[t]\n","            \n","        # Return the predicted outputs from the decoder    \n","        return outputs"]},{"cell_type":"markdown","metadata":{"_cell_guid":"fa634604-d7a6-4119-9da2-8ea5e936789a","_uuid":"a0b20805-c6ce-40b8-9486-f4f709e0b754","trusted":true},"source":["# ENCODER LSTM"]},{"cell_type":"code","execution_count":29,"metadata":{"_cell_guid":"0c7d660e-50a5-4f67-9b37-2899d4782dec","_uuid":"af250e7f-31c4-4231-a67f-1ea1c82ea234","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:55:43.909285Z","iopub.status.busy":"2024-05-16T17:55:43.908988Z","iopub.status.idle":"2024-05-16T17:55:43.921940Z","shell.execute_reply":"2024-05-16T17:55:43.921054Z","shell.execute_reply.started":"2024-05-16T17:55:43.909258Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Encoder module using LSTM (Long Short-Term Memory) cells.\n","class Encoder_LSTM(nn.Module): \n","    #Initializes the Encoder_LSTM module.\n","    def __init__(self, params, preprocessed_data):\n","        super(Encoder_LSTM, self).__init__()\n","        # Initialize dropout layer\n","        self.dropout = nn.Dropout(params['dropout'])\n","        \n","        # Initialize embedding layer\n","        self.embedding = nn.Embedding(len(preprocessed_data['input_index']), params['embedding_size'])\n","        \n","        # Initialize LSTM cell\n","        self.cell = nn.LSTM(params['embedding_size'], params['hidden_size'], params['num_layers_enc'], dropout = params['dropout'], bidirectional = params['bi_dir'])\n","    \n","    #Forward pass of the Encoder_LSTM    \n","    def forward(self, x):\n","        # Embedding layer\n","        drop_par = self.embedding(x)\n","        # applying dropout to the embedding layer\n","        outputs , (hidden, cell) = self.cell(self.dropout(drop_par))\n","        \n","        # Return hidden and cell states\n","        return hidden, cell"]},{"cell_type":"markdown","metadata":{"_cell_guid":"e062a405-ba91-411d-93a8-7f5f0e398640","_uuid":"7c67a515-0c93-4c35-8825-a90604b86678","trusted":true},"source":["# DECODER LSTM"]},{"cell_type":"code","execution_count":30,"metadata":{"_cell_guid":"e77a9a4f-4320-41d1-8f9b-7f53a6a2988f","_uuid":"8eb77a95-663d-486a-933f-e845fa49f77c","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:55:43.925992Z","iopub.status.busy":"2024-05-16T17:55:43.925718Z","iopub.status.idle":"2024-05-16T17:55:43.936385Z","shell.execute_reply":"2024-05-16T17:55:43.935607Z","shell.execute_reply.started":"2024-05-16T17:55:43.925969Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["#Decoder module using LSTM (Long Short-Term Memory) cells.\n","class Decoder_LSTM(nn.Module):\n","    \n","    #Initializes the Decoder_LSTM module\n","    def __init__(self, params, preprocessed_data):\n","        super(Decoder_LSTM, self).__init__()\n","        \n","        # Dropout layer to randomly zero out input elements to prevent overfitting\n","        self.dropout = nn.Dropout(params['dropout'])\n","        \n","        # Embedding layer to convert output tokens into dense vectors\n","        self.embedding = nn.Embedding(len(preprocessed_data['output_index']), params['embedding_size'])\n","        \n","        # LSTM cell for sequence decoding\n","        self.cell =  nn.LSTM(params['embedding_size'], params['hidden_size'], params['num_layers_dec'], dropout = params['dropout'], bidirectional = params['bi_dir'])\n","        self.fc = nn.Linear(params['hidden_size'] *  2 if params['bi_dir'] == True else params['hidden_size'], len(preprocessed_data['output_index']))\n","    \n","    #Forward pass of the Decoder_LSTM.\n","    def forward(self, x, hidden, cell):\n","        \n","        # Embedding layer: maps input token to dense vector\n","        emb = self.embedding(x.unsqueeze(0))\n","        \n","        # Pass the embedded and dropout-processed input through the LSTM cell\n","        outputs , (hidden, cell) = self.cell(self.dropout(emb), (hidden, cell))\n","        \n","        # Predictions with fully connected layer\n","        pred  = self.fc(outputs).squeeze(0)\n","        \n","        # Apply log softmax activation to obtain output probabilities\n","        pred = F.log_softmax(pred, dim = 1)\n","        \n","        # Return predicted output probabilities, updated hidden, and cell states\n","        return pred, hidden, cell"]},{"cell_type":"markdown","metadata":{"_cell_guid":"40a8e1a3-81ce-431d-b4c3-6082e2c53af1","_uuid":"2ddafec7-fd18-4653-a8e5-b99106f97fb8","trusted":true},"source":["# SEQ 2 SEQ LSTM"]},{"cell_type":"code","execution_count":31,"metadata":{"_cell_guid":"e36d8500-68e1-4ead-9020-995c39ea4089","_uuid":"efa01368-e39e-4170-a003-0739bfc44698","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:55:43.937868Z","iopub.status.busy":"2024-05-16T17:55:43.937538Z","iopub.status.idle":"2024-05-16T17:55:43.951450Z","shell.execute_reply":"2024-05-16T17:55:43.950691Z","shell.execute_reply.started":"2024-05-16T17:55:43.937836Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["#Sequence-to-sequence model using LSTM cells for both encoding and decoding.\n","class Seq2Seq_LSTM(nn.Module):\n","    def __init__(self, encoder, decoder, params,  preprocessed_data):\n","        super(Seq2Seq_LSTM, self).__init__()\n","        \n","        # Store references to encoder, decoder, and other attributes\n","        self.cell_type = params['cell_type']\n","        self.decoder, self.encoder  = decoder, encoder\n","        self.output_index_len = len(preprocessed_data['output_index'])\n","        self.tfr = params['teacher_fr']\n","    \n","    #Forward pass of the Seq2Seq_LSTM model.\n","    def forward(self, source, target):\n","        \n","        # Extract batch size and target sequence length\n","        batch_size, target_len = source.shape[1], target.shape[0]\n","        \n","        # Initial input to the decoder (start token)\n","        x = target[0]\n","        outputs = torch.zeros(target_len, batch_size, self.output_index_len).to(device)\n","        \n","        # Encode the source sequence to obtain initial hidden and cell state\n","        hidden, cell = self.encoder(source)\n","        \n","        # Iterate over each step in the target sequence\n","        for t in range(1, target_len):\n","            \n","            # Pass input (x), hidden, and cell states to the decoder\n","            output, hidden, cell = self.decoder(x, hidden, cell)\n","            \n","            # Store the decoder output in the outputs tensor\n","            outputs[t], best_guess = output, output.argmax(1)\n","            \n","            # Determine the next input (x) for the decoder using teacher forcing strategy\n","            x = best_guess if random.random() >= self.tfr else target[t]\n","        \n","        # Return the predicted outputs from the decoder for each time step\n","        return outputs"]},{"cell_type":"markdown","metadata":{"_cell_guid":"8a018960-3a05-406c-9891-4536a7400a18","_uuid":"40a4f7e1-5df4-4932-a202-3abb59b85894","trusted":true},"source":["# GET OPTIMIZERS"]},{"cell_type":"code","execution_count":32,"metadata":{"_cell_guid":"7fc49256-09e5-47d6-9f97-457856a36ea9","_uuid":"de5b9019-e8f2-4f6e-a514-9b4d549e7c99","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:55:43.953050Z","iopub.status.busy":"2024-05-16T17:55:43.952736Z","iopub.status.idle":"2024-05-16T17:55:43.966283Z","shell.execute_reply":"2024-05-16T17:55:43.965572Z","shell.execute_reply.started":"2024-05-16T17:55:43.953021Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Function to get the optimizer based on specified parameters\n","def get_optim(model, params):\n","    # Extract the optimizer type from params and convert to lowercase\n","    val = params['optimizer'].lower()\n","    \n","    if  val== 'sgd':\n","        # Use Stochastic Gradient Descent (SGD) optimizer\n","        opt = optim.SGD(model.parameters(), lr = params['learning_rate'], momentum = 0.9)\n","    \n","    if val == 'adagrad':\n","        # Use adagrad optimizer\n","        opt = optim.Adagrad(model.parameters(), lr = params['learning_rate'], lr_decay = 0, weight_decay = 0, initial_accumulator_value = 0, eps = 1e-10)\n","    \n","    if val == 'adam':\n","        # Use adam optimizer\n","        opt = optim.Adam(model.parameters(), lr = params['learning_rate'], betas = (0.9, 0.999), eps = 1e-8)\n","    \n","    if val == 'rmsprop':\n","        # Use rmsprop optimizer\n","        opt = optim.RMSprop(model.parameters(), lr = params['learning_rate'], alpha = 0.99, eps = 1e-8)\n","    \n","    return opt"]},{"cell_type":"markdown","metadata":{"_cell_guid":"7564374c-3145-4859-b1c3-ec91e5e704bf","_uuid":"a1e85f7b-c019-41d2-a39f-7d37981f4f0f","trusted":true},"source":["# BEAM SEARCH"]},{"cell_type":"code","execution_count":33,"metadata":{"_cell_guid":"fee28368-7fd3-43ee-afa8-9975e9a1c323","_uuid":"3844bf1f-5434-4fc4-957d-e8e4a4f2863c","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:55:43.967750Z","iopub.status.busy":"2024-05-16T17:55:43.967476Z","iopub.status.idle":"2024-05-16T17:55:43.986062Z","shell.execute_reply":"2024-05-16T17:55:43.985266Z","shell.execute_reply.started":"2024-05-16T17:55:43.967728Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["#Beam search function to generate predictions using a sequence-to-sequence model\n","def beam_search(model, word, preprocessed_data, bw, lp, ct):\n","    \n","    # Prepare input data tensor for the model\n","    val=preprocessed_data['max_len']+1\n","    data = np.zeros((val, 1), dtype=np.int32)\n","    \n","    # Map input character to index\n","    idx = 0\n","    while idx < len(word):\n","        char = word[ idx ]\n","        data[ idx , 0 ] = preprocessed_data['input_index'][char]\n","        idx += 1\n","    \n","    # Append EOS token index to indicate end of input sequence\n","    data[idx , 0] = preprocessed_data['input_index'][preprocessed_data['EOS']]\n","    \n","    # Convert input data to torch tensor and move to appropriate device\n","    data = torch.tensor(data, dtype=torch.int32).to(device)\n","    \n","    # Encode the input sequence to obtain initial hidden state\n","    with torch.no_grad():\n","        # For RNN encoder, obtain only hidden state\n","        val = ct !='LSTM' \n","        if  val :\n","            hidden = model.encoder(data)\n","        else:\n","           # For LSTM encoder, obtain both hidden and cell states \n","           hidden, cell = model.encoder(data)\n","    hidden_par = hidden.unsqueeze(0)\n","    \n","    # Reshape the SOS token index for initializing the sequence\n","    out_reshape = np.array(preprocessed_data['output_index'][preprocessed_data['SOS']]).reshape(1,)\n","    initial_seq = torch.tensor(out_reshape).to(device)\n","    \n","    # Initialize beam with the initial sequence and its score\n","    beam = [(0.0, initial_seq, hidden_par)]\n","    \n","    # Beam search loop to generate sequences\n","    i = 0\n","    leng=len(preprocessed_data['output_index'])\n","    while i < leng:\n","        candidates = []\n","        index = 0\n","        while index < len(beam):\n","            score, seq, hidden = beam[index]\n","            \n","            # Check if sequence ends with EOS token\n","            val=seq[-1].item() == preprocessed_data['output_index'][preprocessed_data['EOS']]\n","            if val:\n","                candidates.append((score, seq, hidden))\n","                index+=1\n","                continue\n","            \n","            # Prepare input token for the decoder based on the last token of the sequence\n","            reshape_last = np.array(seq[-1].item()).reshape(1,)\n","            hdn = hidden.squeeze(0)\n","            x = torch.tensor(reshape_last).to(device)\n","            \n","            # Decode the input token to get output probabilities and updated hidden state\n","            val= ct == 'LSTM'\n","            if val!=1:\n","                output ,  hidden = model.decoder(x, hdn, None)\n","            else:\n","                output, hidden , cell = model.decoder(x, hdn, cell)\n","            val=F.softmax(output, dim=1)\n","            \n","            # Apply softmax to obtain probabilities over output tokens\n","            topk_probs , topk_tokens = torch.topk(val, k=bw)\n","            \n","            # Generate candidate sequences based on top-k tokens\n","            ii = 0\n","            while ii < len(topk_probs[0]):\n","                prob = topk_probs[0][ii]\n","                token = topk_tokens[0][ii]\n","                new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n","                \n","                # Calculate length normalization factor (penalty) for the new sequence\n","                ln_ns = len(new_seq)\n","                ln_pf = ((ln_ns - 1) / 5)\n","                candidate_score = score + torch.log(prob).item() / (ln_pf ** lp)\n","                \n","                # Append candidate (score, sequence, hidden state) to candidates list\n","                candidates.append((candidate_score, new_seq, hidden.unsqueeze(0)))\n","                ii += 1\n","            index += 1\n","        # Select top beam width candidates based on scores    \n","        beam = heapq.nlargest(bw, candidates, key=lambda x: x[0])\n","        i += 1\n","    m = max(beam, key=lambda x: x[0]) \n","    _, best_sequence, _ = m\n","    \n","    # Convert predicted sequence tokens to characters and concatenate them\n","    pred = ''.join([preprocessed_data['output_index_rev'][token.item()] for token in best_sequence[1:]])\n","    \n","    # Return the predicted sequence (excluding the EOS token)\n","    return pred[:-1]"]},{"cell_type":"markdown","metadata":{"_cell_guid":"07211640-ef6d-41cc-a48a-b598eb290d37","_uuid":"69a6368a-f523-48cc-b94f-e6d335c4e1f8","trusted":true},"source":["# TRAINING FUNCTION"]},{"cell_type":"code","execution_count":34,"metadata":{"_cell_guid":"7ffc780b-354a-4667-b01a-b9411b6d54c8","_uuid":"cfab93da-a1c2-4caf-8fb5-eccf2d4df674","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:55:43.987748Z","iopub.status.busy":"2024-05-16T17:55:43.987498Z","iopub.status.idle":"2024-05-16T17:55:44.012520Z","shell.execute_reply":"2024-05-16T17:55:44.011662Z","shell.execute_reply.started":"2024-05-16T17:55:43.987727Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Function to train the model\n","def train(model, crit, optimizer, preprocessed_data, tensors, params):\n","    val=1\n","    bs='batch_size'\n","    # Split the input and output data into batches\n","    tr_result = torch.split(tensors['output_data'], params[bs], dim = val)\n","    tr_data=torch.split(tensors['input_data'], params[bs], dim = val)\n","    vl_result= torch.split(tensors['val_output_data'], params[bs], dim=val)\n","    vl_data=torch.split(tensors['val_input_data'], params[bs], dim=val)\n","    \n","    # Loop through epochs\n","    epoch = 0\n","    while epoch < params['num_epochs'] :\n","        epoch +=1\n","        \n","        # Initialize counters for metrics\n","        correct_prediction,total_loss,total_words = 0,0,0\n","        model.train()\n","        leng=len(tr_data)\n","        \n","        # Use tqdm for progress visualization during training\n","        val='Training'\n","        with tqdm(total = leng, desc = val) as pbar:\n","            index = 0\n","            lenn = len(tr_data)\n","            \n","            # Loop through each batch in training data\n","            while index < lenn:\n","                # Move input and target data to device (e.g., GPU)\n","                y = tr_result[index]\n","                x = tr_data[index] \n","                inp_data = x.to(device)\n","                target= y.to(device) \n","                optimizer.zero_grad()\n","                output = model(inp_data, target)\n","                \n","                # Reshape target and output for loss calculation\n","                target = target.reshape(-1)\n","                output = output.reshape(-1, output.shape[2])\n","                \n","                # Create a mask to ignore padding tokens\n","                pad_mask = (target != preprocessed_data['output_index'][preprocessed_data['PAD']])\n","                output = output[pad_mask]\n","                target = target[pad_mask]\n","                \n","                # Compute loss and perform backpropagation\n","                loss = crit(output, target)\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","                optimizer.step()\n","                \n","                # Update metrics\n","                total_loss = total_loss +loss.item()\n","                total_words = total_words + target.size(0)\n","                correct_prediction = correct_prediction + torch.sum(torch.argmax(output, dim=1) == target).item()\n","\n","                index += 1\n","                pbar.update(1)\n","        \n","        # Calculate training accuracy and loss\n","        cal=correct_prediction / total_words\n","        train_accuracy = cal * 100\n","        len_train=len(tr_data)\n","        train_loss = total_loss / len_train\n","        model.eval()\n","        \n","        # Evaluate model on validation data\n","        with torch.no_grad():\n","            val_total_words,val_correct_pred,val_total_loss = 0,0,0\n","            with tqdm(total = len(vl_data), desc = 'Validation') as pbar:\n","                \n","                index = 0\n","                lenn=len(vl_data)\n","                # Loop through each batch in validation data\n","                while index < lenn:\n","                    \n","                    y_val =vl_result[index]\n","                    x_val= vl_data[index]\n","                    # Move validation input and target data to device\n","                    inp_data_val = x_val.to(device)\n","                    target_val=y_val.to(device)\n","                    \n","                    # Forward pass through the model for validation\n","                    output_val = model(inp_data_val, target_val)\n","                    target_val = target_val.reshape(-1)\n","                    output_val = output_val.reshape(-1, output_val.shape[2])\n","                    \n","                    # Create mask to ignore padding tokens\n","                    pad_mask = (target_val != preprocessed_data['output_index'][preprocessed_data['PAD']])\n","                    output_val = output_val[pad_mask]\n","                    target_val = target_val[pad_mask]\n","                    \n","                    # Calculate validation loss and metrics\n","                    val_loss = crit(output_val, target_val)\n","                    val_total_loss = val_total_loss+ val_loss.item()\n","                    val_total_words = val_total_words+ target_val.size(0)\n","                    val_correct_pred = val_correct_pred+ torch.sum(torch.argmax(output_val, dim=1) == target_val).item()\n","                    index += 1\n","                    pbar.update(1)\n","            # Calculate validation accuracy and loss        \n","            cal=val_correct_pred / val_total_words        \n","            val_accuracy = cal * 100\n","            lengg=len(vl_data)\n","            val_loss = val_total_loss / lengg\n","            \n","            # Evaluate model using beam search and calculate word-level accuracy\n","            correct_prediction = 0\n","            total_words = len(preprocessed_data['val_words'])\n","            with tqdm(total = total_words, desc = 'Beam') as pbar_:\n","                index = 0\n","                # Loop through each word in validation set for beam search evaluation\n","                while index < len(preprocessed_data['val_words']):\n","                    word, translation = preprocessed_data['val_words'][index], preprocessed_data['val_translations'][index]\n","                    ans = beam_search(model, word, preprocessed_data, params['beam_width'], params['length_penalty'], params['cell_type'])\n","                    val= translation[1:-1]\n","                    # Check if beam search translation matches reference translation\n","                    if ans == val:\n","                        correct_prediction = correct_prediction +1\n","\n","                    index += 1\n","                    pbar_.update(1)\n","        # Calculate word-level accuracy using beam search            \n","        cal=correct_prediction / total_words\n","        val_accuracy_beam = cal * 100\n","        \n","        # Print and log results\n","        print(f'''Epoch : {epoch}\n","              Train Accuracy Char Level : {train_accuracy:.4f}, Train Loss : {train_loss:.4f}\n","              Validation Accuracy Char Level : {val_accuracy:.4f}, Validation Loss : {val_loss:.4f}\n","              Validation Accuracy Word Level : {val_accuracy_beam:.4f},  Correctly predicted : {correct_prediction}/{total_words}''')\n","        if params['w_log']:\n","            wandb.log(\n","                    {\n","                        'epoch': epoch,\n","                        'training_loss' : train_loss,\n","                        'training_accuracy_char' : train_accuracy,\n","                        'validation_loss' : val_loss,\n","                        'validation_accuracy_char' : val_accuracy,\n","                        'validation_accuracy_word' : val_accuracy_beam,\n","                        'correctly_predicted' : correct_prediction\n","                    }\n","                )\n","    \n","    # Return the trained model and validation accuracies\n","    return model, val_accuracy, val_accuracy_beam"]},{"cell_type":"markdown","metadata":{"_cell_guid":"ced904d4-e5a9-41d6-b679-2734ce2d8c87","_uuid":"f542c8e7-b56d-482a-8bd0-399775c8a3ae","trusted":true},"source":["# HYPERPARAMETERS"]},{"cell_type":"code","execution_count":35,"metadata":{"_cell_guid":"2cb5a6f0-ef01-49e0-9cb9-693d4efb1fb2","_uuid":"b4332dc6-697d-4f6a-b9bb-f1c740661f8f","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:55:44.013781Z","iopub.status.busy":"2024-05-16T17:55:44.013519Z","iopub.status.idle":"2024-05-16T17:55:44.027994Z","shell.execute_reply":"2024-05-16T17:55:44.027098Z","shell.execute_reply.started":"2024-05-16T17:55:44.013759Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["#providing the parameters for the function\n","params = {\n","#     'dataset_path' : r'C:\\Users\\gragh\\OneDrive\\Desktop\\Codes\\CS6910 DL\\Assignment 3\\DataSet\\aksharantar_sampled',\n","    'dataset_path' : '/kaggle/input/dl-ass-3/aksharantar_sampled',\n","    'embedding_size': 256,\n","    'hidden_size': 512,\n","    'num_layers_enc': 3,\n","    'num_layers_dec': 3,\n","    'cell_type': 'LSTM',\n","    'dropout': 0.3,\n","    'optimizer' : 'adagrad',\n","    'learning_rate': 0.01,\n","    'batch_size': 32,\n","    'num_epochs': 10,\n","    'teacher_fr' : 0.7,\n","    'length_penalty' : 0.6,\n","    'beam_width': 1,\n","    'bi_dir' : True,\n","    'w_log' : 0\n","}"]},{"cell_type":"markdown","metadata":{"_cell_guid":"0c373acf-1818-49e9-a978-67da3e905a2b","_uuid":"ff7da804-e5ba-4bdc-a2a7-d470c5095078","trusted":true},"source":["# QUESTION 1 : TRAINING MODEL"]},{"cell_type":"code","execution_count":36,"metadata":{"_cell_guid":"1b37351c-d205-4862-9d14-68b1bb520eb3","_uuid":"edd57dc8-31f8-42e4-ae5b-0deae5865ff5","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:55:44.029363Z","iopub.status.busy":"2024-05-16T17:55:44.029061Z","iopub.status.idle":"2024-05-16T18:30:36.848379Z","shell.execute_reply":"2024-05-16T18:30:36.847458Z","shell.execute_reply.started":"2024-05-16T17:55:44.029339Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [02:38<00:00, 10.11it/s]\n","Validation: 100%|██████████| 128/128 [00:04<00:00, 31.49it/s]\n","Beam: 100%|██████████| 4096/4096 [00:47<00:00, 85.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 1\n","              Train Accuracy Char Level : 55.3069, Train Loss : 1.6294\n","              Validation Accuracy Char Level : 70.4199, Validation Loss : 1.0789\n","              Validation Accuracy Word Level : 28.3203,  Correctly predicted : 1160/4096\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [02:37<00:00, 10.17it/s]\n","Validation: 100%|██████████| 128/128 [00:04<00:00, 31.52it/s]\n","Beam: 100%|██████████| 4096/4096 [00:47<00:00, 86.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 2\n","              Train Accuracy Char Level : 72.5094, Train Loss : 0.9928\n","              Validation Accuracy Char Level : 72.9877, Validation Loss : 1.0035\n","              Validation Accuracy Word Level : 34.3506,  Correctly predicted : 1407/4096\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [02:37<00:00, 10.19it/s]\n","Validation: 100%|██████████| 128/128 [00:04<00:00, 31.65it/s]\n","Beam: 100%|██████████| 4096/4096 [00:47<00:00, 86.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 3\n","              Train Accuracy Char Level : 75.7889, Train Loss : 0.8885\n","              Validation Accuracy Char Level : 74.5673, Validation Loss : 0.9567\n","              Validation Accuracy Word Level : 37.8906,  Correctly predicted : 1552/4096\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [02:37<00:00, 10.15it/s]\n","Validation: 100%|██████████| 128/128 [00:04<00:00, 31.45it/s]\n","Beam: 100%|██████████| 4096/4096 [00:47<00:00, 86.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 4\n","              Train Accuracy Char Level : 77.8222, Train Loss : 0.8238\n","              Validation Accuracy Char Level : 75.2813, Validation Loss : 0.9337\n","              Validation Accuracy Word Level : 39.9414,  Correctly predicted : 1636/4096\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [02:36<00:00, 10.20it/s]\n","Validation: 100%|██████████| 128/128 [00:04<00:00, 31.78it/s]\n","Beam: 100%|██████████| 4096/4096 [00:47<00:00, 86.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 5\n","              Train Accuracy Char Level : 79.2086, Train Loss : 0.7818\n","              Validation Accuracy Char Level : 75.3128, Validation Loss : 0.9476\n","              Validation Accuracy Word Level : 39.9658,  Correctly predicted : 1637/4096\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [02:37<00:00, 10.15it/s]\n","Validation: 100%|██████████| 128/128 [00:04<00:00, 31.52it/s]\n","Beam: 100%|██████████| 4096/4096 [00:47<00:00, 86.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 6\n","              Train Accuracy Char Level : 80.4136, Train Loss : 0.7448\n","              Validation Accuracy Char Level : 75.6841, Validation Loss : 0.9323\n","              Validation Accuracy Word Level : 40.2588,  Correctly predicted : 1649/4096\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [02:37<00:00, 10.14it/s]\n","Validation: 100%|██████████| 128/128 [00:04<00:00, 31.46it/s]\n","Beam: 100%|██████████| 4096/4096 [00:47<00:00, 85.79it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 7\n","              Train Accuracy Char Level : 81.4526, Train Loss : 0.7123\n","              Validation Accuracy Char Level : 75.9326, Validation Loss : 0.9266\n","              Validation Accuracy Word Level : 41.2842,  Correctly predicted : 1691/4096\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [02:37<00:00, 10.17it/s]\n","Validation: 100%|██████████| 128/128 [00:04<00:00, 31.55it/s]\n","Beam: 100%|██████████| 4096/4096 [00:47<00:00, 85.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 8\n","              Train Accuracy Char Level : 82.2051, Train Loss : 0.6889\n","              Validation Accuracy Char Level : 75.5498, Validation Loss : 0.9479\n","              Validation Accuracy Word Level : 41.1865,  Correctly predicted : 1687/4096\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [02:37<00:00, 10.14it/s]\n","Validation: 100%|██████████| 128/128 [00:03<00:00, 32.01it/s]\n","Beam: 100%|██████████| 4096/4096 [00:47<00:00, 86.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 9\n","              Train Accuracy Char Level : 83.0228, Train Loss : 0.6623\n","              Validation Accuracy Char Level : 75.6355, Validation Loss : 0.9552\n","              Validation Accuracy Word Level : 41.2354,  Correctly predicted : 1689/4096\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [02:37<00:00, 10.15it/s]\n","Validation: 100%|██████████| 128/128 [00:04<00:00, 31.51it/s]\n","Beam: 100%|██████████| 4096/4096 [00:47<00:00, 86.36it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch : 10\n","              Train Accuracy Char Level : 83.6507, Train Loss : 0.6439\n","              Validation Accuracy Char Level : 76.2211, Validation Loss : 0.9361\n","              Validation Accuracy Word Level : 42.3096,  Correctly predicted : 1733/4096\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Load preprocessed data based on specified parameters\n","preprocessed_data = loadData(params)\n","\n","# Create tensors from the preprocessed data\n","tensors = create_tensor(preprocessed_data)\n","\n","# Initialize the model based on the cell type specified in parameters\n","if params['cell_type'] == 'LSTM':\n","    # Use LSTM-based encoder, decoder, and Seq2Seq model\n","    encoder = Encoder_LSTM(params, preprocessed_data).to(device)\n","    decoder = Decoder_LSTM(params, preprocessed_data).to(device)\n","    model = Seq2Seq_LSTM(encoder, decoder, params, preprocessed_data).to(device) \n","else:\n","    #Use RNN-based encoder, decoder, and Seq2Seq model \n","    encoder = Encoder(params, preprocessed_data).to(device)\n","    decoder = Decoder(params, preprocessed_data).to(device)\n","    model = Seq2Seq(encoder, decoder, params, preprocessed_data).to(device)  \n","# print(model)\n","\n","# Define the criterion (loss function) for training\n","crit = nn.CrossEntropyLoss(ignore_index = 0)\n","\n","# Get the optimizer based on specified parameters\n","opt = get_optim(model,params)\n","\n","# Initialize Weights & Biases (wandb) if logging is enabled\n","if params['w_log']:\n","    # Set the name of the run based on the model and training parameters\n","    wandb.init(project = 'DL-Assignment-3')\n","    wandb.run.name = f\"c:{params['cell_type']}_e:{params['num_epochs']}_es:{params['embedding_size']}_hs:{params['hidden_size']}_nle:{params['num_layers_enc']}_nld:{params['num_layers_dec']}_o:{params['optimizer']}_lr:{params['learning_rate']}_bs:{params['batch_size']}_tf:{params['teacher_fr']}_lp:{params['length_penalty']}_b:{params['bi_dir']}_bw:{params['beam_width']}\"\n","    \n","# Train the model using the defined criterion, optimizer, and other parameters\n","trained_model, _, _ = train(model, crit, opt, preprocessed_data, tensors, params)\n","\n","# Finish Weights & Biases logging if enabled\n","if params['w_log']:\n","    wandb.finish()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"7e7e0691-c2d7-4ecf-b95d-38b42e98a3df","_uuid":"2fc02eee-07e3-430c-8163-551744f8c862","trusted":true},"source":["# QUESTION 4 : EVALUATE MODEL"]},{"cell_type":"code","execution_count":37,"metadata":{"_cell_guid":"7d00b6be-d01d-4742-b383-ee498af487af","_uuid":"5714bbcb-9586-4df7-b058-69172af973b8","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T18:30:36.849896Z","iopub.status.busy":"2024-05-16T18:30:36.849633Z","iopub.status.idle":"2024-05-16T18:31:27.632086Z","shell.execute_reply":"2024-05-16T18:31:27.631257Z","shell.execute_reply.started":"2024-05-16T18:30:36.849873Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 4096/4096 [00:50<00:00, 80.72it/s]"]},{"name":"stdout","output_type":"stream","text":["Test Accuracy Word Level : 40.4296875, Correctly Predicted : 1656\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Word</th>\n","      <th>Translation</th>\n","      <th>Prediction</th>\n","      <th>Result</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>thermax</td>\n","      <td>थरमैक्स</td>\n","      <td>थर्मक्स</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>sikhaaega</td>\n","      <td>सिखाएगा</td>\n","      <td>सिखाएगा</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>learn</td>\n","      <td>लर्न</td>\n","      <td>लीर्न</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>twitters</td>\n","      <td>ट्विटर्स</td>\n","      <td>ट्विटर्स</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>tirunelveli</td>\n","      <td>तिरुनेलवेली</td>\n","      <td>तिरुनेलवेली</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4091</th>\n","      <td>saflata</td>\n","      <td>सफ़लता</td>\n","      <td>सफलता</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>4092</th>\n","      <td>shbana</td>\n","      <td>शबाना</td>\n","      <td>शबाना</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>4093</th>\n","      <td>khaatootolaa</td>\n","      <td>खातूटोला</td>\n","      <td>खाटूटोला</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>4094</th>\n","      <td>shivastava</td>\n","      <td>शिवास्तव</td>\n","      <td>शिवास्तव</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>4095</th>\n","      <td>preranapuree</td>\n","      <td>प्रेरणापुरी</td>\n","      <td>प्रेराणपुरी</td>\n","      <td>No</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4096 rows × 4 columns</p>\n","</div>"],"text/plain":["              Word  Translation   Prediction Result\n","0          thermax      थरमैक्स      थर्मक्स     No\n","1        sikhaaega      सिखाएगा      सिखाएगा    Yes\n","2            learn         लर्न        लीर्न     No\n","3         twitters     ट्विटर्स     ट्विटर्स    Yes\n","4      tirunelveli  तिरुनेलवेली  तिरुनेलवेली    Yes\n","...            ...          ...          ...    ...\n","4091       saflata       सफ़लता        सफलता     No\n","4092        shbana        शबाना        शबाना    Yes\n","4093  khaatootolaa     खातूटोला     खाटूटोला     No\n","4094    shivastava     शिवास्तव     शिवास्तव    Yes\n","4095  preranapuree  प्रेरणापुरी  प्रेराणपुरी     No\n","\n","[4096 rows x 4 columns]"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["\n","# Set the trained model to evaluation mode\n","trained_model.eval()\n","\n","# Initialize variables for tracking predictions and evaluation\n","correct_prediction = 0\n","words = []\n","translations = []\n","prediction = []\n","results = []\n","\n","# Use tqdm to visualize progress during inference\n","total_words = len(preprocessed_data['test_words'])\n","with tqdm(total = total_words) as pbar_:\n","\n","    # Loop through each word in the test set\n","    index = 0\n","    while index < len(preprocessed_data['test_words']):\n","        word, translation = preprocessed_data['test_words'][index], preprocessed_data['test_translations'][index]\n","\n","        # Perform beam search to generate a translation using the trained model\n","        ans = beam_search(trained_model, word, preprocessed_data, params['beam_width'], params['length_penalty'], params['cell_type'])\n","\n","        # Store the word (without end token), translation (without start/end tokens), and predicted translation\n","        words.append(word[:-1])\n","        translations.append(translation[1:-1])\n","        prediction.append(ans)\n","\n","        # Check if the predicted translation matches the reference translation\n","        val= ans == translation[1:-1]\n","        if val!=1 :\n","            results.append('No')\n","        else:\n","            correct_prediction = correct_prediction + 1\n","            results.append('Yes')\n","        index += 1\n","        pbar_.update(1)\n","\n","# Calculate accuracy based on correct predictions  \n","cal=correct_prediction / total_words    \n","accuracy = cal * 100\n","print(f'Test Accuracy Word Level : {accuracy}, Correctly Predicted : {correct_prediction}')\n","\n","# Prepare a dictionary for logging predictions\n","log = {'Word': words, 'Translation' : translations, 'Prediction' : prediction, 'Result' : results}\n","path = '/kaggle/working/predictions_vanilla.csv'\n","\n","# Create a DataFrame from the logging dictionary and save it as a CSV file\n","data_frame = pd.DataFrame(log)\n","data_frame.to_csv(path, header = True, index = False)\n","\n","# Optionally display the DataFrame (for debugging or verification)\n","pd.DataFrame(log)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"5f335593-8c44-4dac-b096-a58095a9c7ad","_uuid":"92ec3914-85d0-4e34-a4f8-0495fbe36a24","trusted":true},"source":["# QUESTION 2 : Tuning Hyperparameters"]},{"cell_type":"code","execution_count":38,"metadata":{"_cell_guid":"86a7c785-1c89-4356-93a7-9b60444d0085","_uuid":"e71869d6-6a48-44c8-90ee-4fe700c06c25","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T18:31:27.633784Z","iopub.status.busy":"2024-05-16T18:31:27.633403Z","iopub.status.idle":"2024-05-16T18:31:27.639461Z","shell.execute_reply":"2024-05-16T18:31:27.638388Z","shell.execute_reply.started":"2024-05-16T18:31:27.633747Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# # Define individual hyperparameter values or ranges\n","# ne= {'values': [10]}\n","# ct = {'values': ['RNN', 'LSTM', 'GRU']}\n","# es = {'values': [128, 256, 512]}\n","# nl= {'values': [1, 2, 3]}\n","# dp= {'values': [0.3, 0.5, 0.7]}\n","# lr= {'values': [0.001, 0.005, 0.01, 0.1]}\n","# bs= {'values': [32, 64]}\n","# lp= {'values': [0.4, 0.5, 0.6]}\n","# bi= {'values': [True, False]}\n","# hs= {'values': [128, 256, 512]}\n","# opt={'values' : ['adam', 'sgd', 'rmsprop', 'adagrad']}\n","# bw = {'values': [1, 2, 3]}\n","# tf ={'values': [0.3, 0.5, 0.7]}\n","\n","# # Define the sweep configuration dictionary\n","# sweep_config = {\n","#             'name': 'sweep 1 and 1.1 : random',\n","#             'method': 'random',\n","#             'metric': { 'goal': 'maximize','name': 'Accuracy'},\n","#             'parameters': \n","#                 {\n","#                     'num_epochs': ne,\n","#                     'cell_type': ct,\n","#                     'embedding_size': es,\n","#                     'hidden_size': hs,\n","#                     'num_layers': nl ,\n","#                     'dropout': dp,\n","#                     'optimizer' : opt,\n","#                     'learning_rate': lr,\n","#                     'batch_size': bs,\n","#                     'teacher_fr' : tf,\n","#                     'length_penalty' : lp,\n","#                     'bi_dir' :bi ,\n","#                     'beam_width': bw\n","#                 }\n","#             }"]},{"cell_type":"code","execution_count":39,"metadata":{"_cell_guid":"f04e2c4c-a4e1-4fff-a9dc-2b1ac0ccf13e","_uuid":"5867e8f1-5c7d-4844-8a2c-11b47027e3a5","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T18:31:27.641487Z","iopub.status.busy":"2024-05-16T18:31:27.640873Z","iopub.status.idle":"2024-05-16T18:31:27.656355Z","shell.execute_reply":"2024-05-16T18:31:27.655527Z","shell.execute_reply.started":"2024-05-16T18:31:27.641454Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# # Function to run sweep\n","# def run_sweep():\n","#     #Initialize Weights & Biases run for experiment tracking\n","#     init = wandb.init(project = 'DL-Assignment-3')\n","#     config = init.config\n","#     # Define parameters based on the configuration from Weights & Biases\n","#     params = {\n","#         'language' : 'hin',\n","#         'dataset_path' : '/kaggle/input/dl-ass3/aksharantar_sampled',\n","#         'num_epochs': config.num_epochs,\n","#         'cell_type': config.cell_type,\n","#         'embedding_size': config.embedding_size,\n","#         'hidden_size': config.hidden_size,\n","#         'num_layers_enc': config.num_layers,\n","#         'num_layers_dec': config.num_layers,\n","#         'dropout': config.dropout,\n","#         'optimizer' : config.optimizer,\n","#         'learning_rate': config.learning_rate,\n","#         'batch_size': config.batch_size,\n","#         'teacher_fr' : config.teacher_fr,\n","#         'length_penalty' : config.length_penalty,\n","#         'bi_dir' : config.bi_dir,\n","#         'beam_width' : config.beam_width,\n","#         'w_log' : 1\n","#     }\n","    \n","#     # Set the name of the Weights & Biases run based on the experiment configuration\n","#     wandb.run.name = f\"c:{params['cell_type']}_e:{params['num_epochs']}_es:{params['embedding_size']}_hs:{params['hidden_size']}_nle:{params['num_layers_enc']}_nld:{params['num_layers_dec']}_o:{params['optimizer']}_lr:{params['learning_rate']}_bs:{params['batch_size']}_tf:{params['teacher_fr']}_lp:{params['length_penalty']}_b:{params['bi_dir']}_bw:{params['beam_width']}\"\n","#     preprocessed_data = loadData(params)\n","#     tensors = create_tensor(preprocessed_data)\n","    \n","#     # Initialize the decoder, encoder, and seq2seq model based on the parameters\n","#     decoder = Decoder(params, preprocessed_data).to(device)\n","#     encoder = Encoder(params, preprocessed_data).to(device)\n","#     model = Seq2Seq(encoder, decoder, params, preprocessed_data).to(device) \n","    \n","#     # Define the loss function (criterion) and optimizer based on the model and parameters\n","#     crit = nn.CrossEntropyLoss(ignore_index = 0)\n","#     opt = get_optim(model,params)\n","    \n","#     # Perform training and obtain validation accuracy with beam search\n","#     _, _, v_acc_beam = train(model, crit, opt, preprocessed_data, tensors, params)\n","    \n","#     # Log the validation accuracy with beam search to Weights & Biases\n","#     wandb.log({'Accuracy': v_acc_beam})"]},{"cell_type":"code","execution_count":40,"metadata":{"_cell_guid":"a35dabc5-6e89-446b-9efe-5dba3035a3c7","_uuid":"2978c94b-fc4e-464c-9c2b-43d8ea24fb3f","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T18:31:27.657795Z","iopub.status.busy":"2024-05-16T18:31:27.657465Z","iopub.status.idle":"2024-05-16T18:31:27.669733Z","shell.execute_reply":"2024-05-16T18:31:27.668890Z","shell.execute_reply.started":"2024-05-16T18:31:27.657763Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# # Initiate a hyperparameter sweep and obtain the sweep ID\n","# sweep_id = wandb.sweep(sweep_config, project='DL-Assignment-3')\n","\n","# # Run the hyperparameter sweep using the defined `run_sweep` function as an agent\n","# # `count=25` specifies the number of runs (trials) to execute for the sweep\n","# wandb.agent(sweep_id, run_sweep, count = 30)\n","\n","# # Finish the Weights & Biases run after completing the hyperparameter sweep\n","# wandb.finish()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4933120,"sourceId":8304200,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
